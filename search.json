[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML23",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 4803/5803 Machine Learning Fall 2023 at ATU. If you have any comments/suggetions/concers about the notes please contact us at xxiao@atu.edu.\n\n\nReferences"
  },
  {
    "objectID": "contents/1/intro.html",
    "href": "contents/1/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In this Chapter we will discuss\n\nWhat is Machine Learning?\nWhat do typical Machine Learning problems look like?\nWhat is the basic structure of Machine Learning models?\nWhat is the basic work flow to use Machine Learning to solve problems?\nSome supplementary materials, such as Linear Algebra and Python."
  },
  {
    "objectID": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "href": "contents/2/intro.html#k-nearest-neighbors-algorithm-k-nn",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.1 k-Nearest Neighbors Algorithm (k-NN)",
    "text": "2.1 k-Nearest Neighbors Algorithm (k-NN)\n\n2.1.1 Ideas\nAssume that we have a set of labeled data \\(\\{(X_i, y_i)\\}\\) where \\(y_i\\) denotes the label. Given a new data \\(X\\), how do we determine the label of it?\nk-NN algorithm starts from a very straightforward idea. We use the distances from the new data point \\(X\\) to the known data points to identify the label. If \\(X\\) is closer to \\(y_i\\) points, then we will label \\(X\\) as \\(y_i\\).\nLet us take cities and countries as an example. New York and Los Angeles are U.S cities, and Beijing and Shanghai are Chinese cities. Now we would like to consider Tianjin and Russellville. Do they belong to China or U.S? We calculate the distances from Tianjin (resp. Russellville) to all four known cities. Since Tianjin is closer to Beijing and Shanghai comparing to New York and Los Angeles, we classify Tianjin as a Chinese city. Similarly, since Russellville is closer to New York and Los Angeles comparing to Beijing and Shanghai, we classify it as a U.S. city.\n\n\n\n\n\n\n\nG\n\n  \n\nBeijing\n\n Beijing   \n\nShanghai\n\n Shanghai   \n\nTianjin\n\n Tianjin   \n\nTianjin-&gt;Beijing\n\n  closer   \n\nTianjin-&gt;Shanghai\n\n  closer      \n\nNew York\n\n New York   \n\nTianjin-&gt;New York\n\n   far away   \n\nLos Angelis\n\n Los Angelis   \n\nTianjin-&gt;Los Angelis\n\n   far away   \n\nRussellville\n\n Russellville   \n\nRussellville-&gt;Beijing\n\n  far away    \n\nRussellville-&gt;Shanghai\n\n  far away   \n\nRussellville-&gt;New York\n\n  closer     \n\nRussellville-&gt;Los Angelis\n\n  closer  \n\n\n\n\n\nThis naive example explains the idea of k-NN. Here is a more detailed description of the algorithm.\n\n\n2.1.2 The Algorithm\nk-NN Classifier Inputs Given the training data set \\(\\{(X_i, y_i)\\}\\) where \\(X_i=(x_i^1,x_i^2,\\ldots,x_i^n)\\) represents \\(n\\) features and \\(y_i\\) represents labels. Given a new data point \\(\\tilde{X}=(\\tilde{x}^1,\\tilde{x}^2,\\ldots,\\tilde{x}^n)\\).\nOutputs Want to find the best label for \\(\\tilde{X}\\).\n\nCompute the distance from \\(\\tilde{X}\\) to each \\(X_i\\).\nSort all these distances from the nearest to the furthest.\nFind the nearest \\(k\\) data points.\nDetermine the labels for each of these \\(k\\) nearest points, and compute the frenqucy of each labels.\nThe most frequent label is considered to be the label of \\(\\tilde{X}\\).\n\n\n\n2.1.3 Details\n\nThe distance between two data points are defined by the Euclidean distance:\n\n\\[\ndist\\left((x^j_i)_{j=1}^n, (\\tilde{x}^j)_{j=1}^n\\right)=\\sqrt{\\sum_{j=1}^n(x^j_i-\\tilde{x}^j)^2}.\n\\]\n\nUsing linear algebra notations:\n\n\\[\ndist(X_i,\\tilde{X})=\\sqrt{(X_i-\\tilde{X})\\cdot(X_i-\\tilde{X})}.\n\\]\n\nAll the distances are stored in a \\(1\\)-dim numpy array, and we will combine it together with another \\(1\\)-dim array that store the labels of each point.\n\n\n\n2.1.4 The codes\n\nargsort\nget\nsorted\n\n\ndef classify_kNN(inX, X, y, k):\n    # create a new 2-d numpy array by copying inX for each row.\n    Xmat = np.tile(np.array([inX]), (X.shape[0], 1))\n    # compute the distance between each row of X and Xmat\n    Dmat = np.sqrt(np.sum((Xmat - X)**2, axis=1))\n    # sort by distance\n    sortedlist = Dmat.argsort()\n    # count the freq. of the first k items\n    k = min(k, len(sortedlist))\n    classCount = dict()\n    for i in sortedlist[:k]:\n        classCount[y[i]] = classCount.get(y[i], 0) + 1\n    # find out the most freqent one\n    sortedCount = sorted(classCount.items(), key=lambda x:x[1],\n                         reverse=True)\n    return sortedCount[0][0]\n\n\n\n2.1.5 sklearn packages\nYou may also directly use the kNN function KNeighborsClassifier packaged in sklearn.neighbors. You may check the description of the function online from here.\nThere are many ways to modify the kNN algorithm. What we just mentioned is the simplest idea. It is correspondent to the argument weights='uniform', algorithm='brute and metric='euclidean'. However due to the implementation details, the results we got from sklearn are still a little bit different from the results produced by our naive codes.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='brute',\n                             metric='euclidean')\nclf.fit(X_train, y_train)\ny_pred = ckf.predict(X_test)\n\n\n\n2.1.6 Normalization\nDifferent features may have different scales. It might be unfair for those features that have small scales. Therefore usually it is better to rescale all the features to make them have similar scales. After examining all the data, we find the minimal value minVal and the range ranges for each column. The normalization formula is:\n\\[\nX_{norm} = \\frac{X_{original}-minVal}{ranges}.\n\\]\nWe could also convert the normalized number back to the original value by\n\\[\nX_{original} = X_{norm} \\times ranges + minVal.\n\\]\nThe sample codes are listed below.\n\ndef encodeNorm(X, parameters=None):\n    # parameters contains minVals and ranges\n    if parameters is None:\n        minVals = np.min(X, axis=0)\n        maxVals = np.max(X, axis=0)\n        ranges = np.maximum(maxVals - minVals, np.ones(minVals.size))\n        parameters = {'ranges': ranges, 'minVals': minVals}\n    else:\n        minVals = parameters['minVals']\n        ranges = parameters['ranges']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xnorm = (X - Nmat)/ranges\n    return (Xnorm, parameters)\n\n\ndef decodeNorm(X, parameters):\n    # parameters contains minVals and ranges\n    ranges = parameters['ranges']\n    minVals = parameters['minVals']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xoriginal = X * ranges + Nmat\n    return Xoriginal"
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "href": "contents/2/intro.html#k-nn-project-1-iris-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.2 k-NN Project 1: iris Classification",
    "text": "2.2 k-NN Project 1: iris Classification\nThis data is from sklearn.datasets. This dataset consists of 3 different types of irises’ petal / sepal length / width, stored in a \\(150\\times4\\) numpy.ndarray. We already explored the dataset briefly in the previous chapter. This time we will try to use the feature provided to predict the type of the irises. For the purpose of plotting, we will only use the first two features: sepal length and sepal width.\n\n2.2.1 Explore the dataset\nWe first load the dataset.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nThen we would like to split the dataset into trainning data and test data. Here we are going to use sklearn.model_selection.train_test_split function. Besides the dataset, we should also provide the propotion of the test set comparing to the whole dataset. We will choose test_size=0.1 here, which means that the size of the test set is 0.1 times the size of the whole dataset. stratify=y means that when split the dataset we want to split respects the distribution of labels in y.\nThe split will be randomly. You may set the argument random_state to be a certain number to control the random process. If you set a random_state, the result of the random process will stay the same. This is for reproducible output across multiple function calls.\nAfter we get the training set, we should also normalize it. All our normalization should be based on the training set. When we want to use our model on some new data points, we will use the same normalization parameters to normalize the data points in interests right before we apply the model. Here since we mainly care about the test set, we could normalize the test set at this stage.\nNote that in the following code, I import functions encodeNorm from assests.codes.knn. You need to modify this part based on your file structure. See here for more details.\n\nfrom sklearn.model_selection import train_test_split\nfrom assests.codes.knn import encodeNorm\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\nBefore we start to play with k-NN, let us look at the data first. Since we only choose two features, it is able to plot these data points on a 2D plane, with different colors representing different classes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the scatter plot.\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n# Generate legends.\nlabels = ['setosa', 'versicolor', 'virginica']\nfig.legend(handles=scatter.legend_elements()[0], labels=labels,\n           loc=\"right\", title=\"Labels\")\n\n# plt.show()\n\n&lt;matplotlib.legend.Legend at 0x1feaa993850&gt;\n\n\n\n\n\n(section:applyourknn)= ### Apply our k-NN model\nNow let us apply k-NN to this dataset. We first use our codes. Here I use from assests.codes.knn to import our functions since I put all our functions in ./assests/codes/knn.py. Then the poential code is\ny_pred = classify_kNN(X_test, X_train, y_train, k=10)\nNote that the above code is actually wrong. The issue ist that our function classify_kNN can only classify one row of data. To classify many rows, we need to use a for loop.\n\nfrom assests.codes.knn import classify_kNN\n\nn_neighbors = 10\ny_pred = list()\nfor row in X_test_norm:\n    row_pred = classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n    y_pred.append(row_pred)\ny_pred = np.array(y_pred)\n\nWe could use list comprehension to simply the above codes.\n\nfrom assests.codes.knn import classify_kNN\n\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nThis y_pred is the result we got for the test set. We may compare it with the real answer y_test, and calcuate the accuracy.\n\nacc = np.mean(y_pred == y_test)\nprint(acc)\n\n0.7333333333333333\n\n\n\n\n2.2.2 Apply k-NN model from sklearn\nNow we would like to use sklearn to reproduce this result. Since our data is prepared, what we need to do is directly call the functions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 10\nclf = KNeighborsClassifier(n_neighbors, weights=\"uniform\", metric=\"euclidean\",\n                           algorithm='brute')\nclf.fit(X_train_norm, y_train)\ny_pred_sk = clf.predict(X_test_norm)\n\nacc = np.mean(y_pred_sk == y_test)\nprint(acc)\n\n0.7333333333333333\n\n\n\n\n2.2.3 Using data pipeline\nWe may organize the above process in a neater way. After we get a data, the usual process is to apply several transforms to the data before we really get to the model part. Using terminolgies from sklearn, the former are called transforms, and the latter is called an estimator. In this example, we have exactly one tranform which is the normalization. The estimator here we use is the k-NN classifier.\nsklearn provides a standard way to write these codes, which is called pipeline. We may chain the transforms and estimators in a sequence and let the data go through the pipeline. In this example, the pipeline contains two steps: 1. The normalization transform sklearn.preprocessing.MinMaxScaler. When we directly apply it the parameters ranges and minVals and will be recorded automatically, and we don’t need to worry about it when we want to use the same parameters to normalize other data. 2. The k-NN classifier sklearn.neighbors.KNeighborsClassifier. This is the same one as we use previously.\nThe code is as follows. It is a straightforward code. Note that the () after the class in each step of steps is very important. The codes cannot run if you miss it.\nAfter we setup the pipeline, we may use it as other estimators since it is an estimator. Here we may also use the accuracy function provided by sklearn to perform the computation. It is essentially the same as our acc computation.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\nprint(accuracy_score(y_pipe, y_test))\n\n0.7333333333333333\n\n\n\n\n2.2.4 Visualize the Decision boundary\nUsing the classifier we get above, we are able to classify every points on the plane. This enables us to draw the following plot, which is called the Decision boundary. It helps us to visualize the relations between features and the classes.\nWe use DecisionBoundaryDisplay from sklearn.inspection to plot the decision boundary. The function requires us to have a fitted classifier. We may use the classifier pipe we got above. Note that this classifier should have some build-in structures that our classify_kNN function doesn’t have. We may rewrite our codes to make it work, but this goes out of the scope of this section. This is supposed to be Python programming exercise. We will talk about it in the future if we have enough time.\nWe first plot the dicision boundary using DecisionBoundaryDisplay.from_estimator. Then we plot the points from X_test. From the plot it is very clear which points are misclassified.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n            pipe, \n            X_train,\n            response_method=\"predict\",\n            plot_method=\"pcolormesh\",\n            xlabel=iris.feature_names[0],\n            ylabel=iris.feature_names[1],\n            alpha=0.5)\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\")\ndisp.figure_.set_size_inches((10,7))\n\n\n\n\n(section-cross-validation)= ### k-Fold Cross-Validation\nPreviously we perform a random split and test our model in this case. What would happen if we fit our model on another split? We might get a different accuracy score. So in order to evaluate the performance of our model, it is natual to consider several different split and compute the accuracy socre for each case, and combine all these socres together to generate an index to indicate whehter our model is good or bad. This naive idea is called k-Fold Cross-Validation.\nThe algorithm is described as follows. We first randomly split the dataset into k groups. We use one of them as the test set, and the rest together forming the training set, and use this setting to get an accuracy score. We did this for each group to be chosen as the test set. Then the final score is the mean.\nsklearn provides a function sklearn.model_selection.cross_val_score to perform the above computation. The usage is straightforward, as follows.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(pipe, X, y, cv=5)\nprint(cv_scores)\nprint(np.mean(cv_scores))\n\n[0.66666667 0.8        0.63333333 0.8        0.7       ]\n0.7200000000000001\n\n\n\n\n2.2.5 Choosing a k value\nIn the previous example we choose k to be 10 as an example. To choose a k value we usually run some test by trying different k and choose the one with the best performance. In this case, best performance means the highest cross-validation score.\nsklearn.model_selection.GridSearchCV provides a way to do this directly. We only need to setup the esitimator, the metric (which is the cross-validation score in this case), and the hyperparameters to be searched through, and GridSearchCV will run the search automatically.\nWe let k go from 1 to 100. The code is as follows.\nNote that parameters is where we set the search space. It is a dictionary. The key is the name of the estimator plus double _ and then plus the name of the parameter.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nprint(clf.best_estimator_.get_params()[\"knn__n_neighbors\"])\n\n35\n\n\nAfter we fit the data, the best_estimator_.get_params() can be printed. It tells us that it is best to use 31 neibhours for our model. We can directly use the best estimator by calling clf.best_estimator_.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nprint(np.mean(cv_scores))\n\n0.82\n\n\nThe cross-validation score using k=31 is calculated. This serves as a benchmark score and we may come back to dataset using other methods and compare the scores."
  },
  {
    "objectID": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "href": "contents/2/intro.html#k-nn-project-2-dating-classification",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.3 k-NN Project 2: Dating Classification",
    "text": "2.3 k-NN Project 2: Dating Classification\nThe data can be downloaded from {Download}here&lt;./assests/datasets/datingTestSet2.txt&gt;.\n\n2.3.1 Background\nHelen dated several people and rated them using a three-point scale: 3 is best and 1 is worst. She also collected data from all her dates and recorded them in the file attached. These data contains 3 features:\n\nNumber of frequent flyer miles earned per year\nPercentage of time spent playing video games\nLiters of ice cream consumed per week\n\nWe would like to predict her ratings of new dates when we are given the three features.\nThe data contains four columns, while the first column refers to Mileage, the second Gamingtime, the third Icecream and the fourth Rating.\n\n\n2.3.2 Look at Data\nWe first load the data and store it into a DataFrame.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('./assests/datasets/datingTestSet2.txt', sep='\\t', header=None)\ndf.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\nTo make it easier to read, we would like to change the name of the columns.\n\ndf = df.rename(columns={0: \"Mileage\", 1: \"Gamingtime\", 2: 'Icecream', 3: 'Rating'})\ndf.head()\n\n\n\n\n\n\n\n\nMileage\nGamingtime\nIcecream\nRating\n\n\n\n\n0\n40920\n8.326976\n0.953952\n3\n\n\n1\n14488\n7.153469\n1.673904\n2\n\n\n2\n26052\n1.441871\n0.805124\n1\n\n\n3\n75136\n13.147394\n0.428964\n1\n\n\n4\n38344\n1.669788\n0.134296\n1\n\n\n\n\n\n\n\nSince now we have more than 2 features, it is not suitable to directly draw scatter plots. We use seaborn.pairplot to look at the pairplot. From the below plots, before we apply any tricks, it seems that Milegae and Gamingtime are better than Icecream to classify the data points.\n\nimport seaborn as sns\nsns.pairplot(data=df, hue='Rating')\n\n\n\n\n\n\n2.3.3 Applying kNN\nSimilar to the previous example, we will apply both methods for comparisons.\n\nfrom sklearn.model_selection import train_test_split\nfrom assests.codes.knn import encodeNorm\nX = np.array(df[['Mileage', 'Gamingtime', 'Icecream']])\ny = np.array(df['Rating'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\n\n# Using our codes.\nfrom assests.codes.knn import classify_kNN\n\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nacc = np.mean(y_pred == y_test)\nprint(acc)\n\n0.93\n\n\n\n# Using sklearn.\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\nprint(accuracy_score(y_pipe, y_test))\n\n0.93\n\n\n\n\n2.3.4 Choosing k Value\nSimilar to the previous section, we can run tests on k value to choose one to be used in our model using GridSearchCV.\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nprint(clf.best_estimator_.get_params()[\"knn__n_neighbors\"])\n\n4\n\n\nFrom this result, in this case the best k is 4. The corresponding cross-validation score is computed below.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nprint(np.mean(cv_scores))\n\n0.952"
  },
  {
    "objectID": "contents/2/intro.html#exercises-and-projects",
    "href": "contents/2/intro.html#exercises-and-projects",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "2.4 Exercises and Projects",
    "text": "2.4 Exercises and Projects\n````qwujfmyzjc Handwritten example :label: ex2handwritten Consider the 1-dimensional data set shown below.\n```xbycpohtzqdi Dataset :header-rows: 1\n\n\n\\(x\\)\n1.5\n2.5\n3.5\n4.5\n5.0\n5.5\n5.75\n6.5\n7.5\n10.5\n\n\n\\(y\\)\n\\(+\\)\n\\(+\\)\n\\(-\\)\n\\(-\\)\n\\(-\\)\n\\(+\\)\n\\(+\\)\n\\(-\\)\n\\(+\\)\n\\(+\\)\n\nPlease use the data to compute the class of $x=5.5$ according to $k=1$, $3$, $6$ and $9$. Please compute everything by hand.\ntlixqwxxsr ex2handwritten :class: dropdown Not yet done!\n\n:label: ex2titanic\nPlease download the titanic dataset from {Download}`here&lt;./assests/datasets/titanic.csv&gt;`. This is the same dataset from what you dealt with in Chapter 1 Exercises. Therefore you may use the same way to prepare the data. \n\nPlease analyze the dataset and build a k-NN model to predict whether someone is survived or not. Note that you have to pick `k` at the end.\n````tlixqwxxsr ex2titanic :class: dropdown\nNot yet done! ````"
  }
]