[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ML23",
    "section": "",
    "text": "Preface\nThis is the lecture notes for STAT 2304 Programming languages for Data Science Spring 2023 at ATU. If you have any comments/suggetions/concers about the notes please contact us at xxiao@atu.edu or wjia@atu.edu."
  },
  {
    "objectID": "contents/1/intro.html",
    "href": "contents/1/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 What is Machine Learning?\nMachine Learning is the science (and art) of programming computers so they can learn from data {cite:p}Ger2019.\nHere is a slightly more general definition:\nThis “without being explicitly programmed to do so” is the essential difference between Machine Learning and usual computing tasks. The usual way to make a computer do useful work is to have a human programmer write down rules — a computer program — to be followed to turn input data into appropriate answers. Machine Learning turns this around: the machine looks at the input data and the expected task outcome, and figures out what the rules should be. A Machine Learning system is trained rather than explicitly programmed. It’s presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task {cite:p}Cho2021.\nThese exercises are from {cite:p}Klo2021, {cite:p}Ger2019 and {cite:p}Har2012."
  },
  {
    "objectID": "contents/1/intro.html#types-of-machine-learning-systems",
    "href": "contents/1/intro.html#types-of-machine-learning-systems",
    "title": "1  Introduction",
    "section": "2.1 Types of Machine Learning Systems",
    "text": "2.1 Types of Machine Learning Systems\nThere are many different types of Machine Learning systems that it is useful to classify them in braod categories, based on different criteria. These criteria are not exclusive, and you can combine them in any way you like.\nThe most popular criterion for Machine Learning classification is the amount and type of supervision they get during training. In this case there are four major types.\nSupervised Learning\n    The training set you feed to the algorithm includes the desired solutions. The machines learn from the data to alter the model to get the desired output. The main task for Supervised Learning is classification and regression.\n\nUnsupervised Learning\n    In Unsupervised Learning, the data provided doesn't have class information or desired solutions. We just want to dig some information directly from those data themselves. Usually Unsupervised Learning is used for clustering and dimension reduction.\n\nReinforcement Learning\n    In Reinforcement Learning, there is a reward system to measure how well the machine performs the task, and the machine is learning to find the strategy to maximize the rewards. Typical examples here include gaming AI and walking robots.\n\nSemisupervised Learning\n    This is actually a combination of Supervised Learning and Unsupervised Learning, that it is usually used to deal with data that are half labelled. \n\n2.1.1 Tasks for Supervised Learning\nAs mentioned above, for Supervised Learning, there are two typical types of tasks:\nClassification\n    It is the task of predicting a discrete class labels. A typical classification problem is to see an handwritten digit image and recognize it.\n\nRegression\n    It is the task of predicting a continuous quantity. A typical regression problem is to predict the house price based on various features of the house.\nThere are a lot of other tasks that are not directly covered by these two, but these two are the most classical Supervised Learning tasks.\nIn this course we will mainly focus on **Supervised Classification problems**.\n\n\n2.1.2 Classification based on complexity\nAlong with the popularity boost of deep neural network, there comes another classificaiton: shallow learning vs. deep learning. Basically all but deep neural network belongs to shallow learning. Although deep learning can do a lot of fancy stuffs, shallow learning is still very good in many cases. When the performance of a shallow learning model is good enough comparing to that of a deep learning model, people tend to use the shallow learning since it is usually faster, easier to understand and easier to modify."
  },
  {
    "objectID": "contents/1/intro.html#input-and-output-data-structure",
    "href": "contents/1/intro.html#input-and-output-data-structure",
    "title": "1  Introduction",
    "section": "3.1 Input and output data structure",
    "text": "3.1 Input and output data structure\nSince we are dealing with Supervised Classification problems, the desired solutions are given. These desired solutions in Classification problems are also called labels. The properties that the data are used to describe are called features. Both features and labels are usually organized as row vectors.\n\nExample 3.1 The example is extracted from {cite:p}Har2012. There are some sample data shown in the following table. We would like to use these information to classify bird species.\n```{list-table} Bird species classification based on four features :header-rows: 1\n\n\nWeight (g)\nWingspan (cm)\nWebbed feet?\nBack color\nSpecies\n\n\n1000.1\n125.0\nNo\nBrown\nButeo jamaicensis\n\n\n3000.7\n200.0\nNo\nGray\nSagittarius serpentarius\n\n\n3300.0\n220.3\nNo\nGray\nSagittarius serpentarius\n\n\n4100.0\n136.0\nYes\nBlack\nGavia immer\n\n\n3.0\n11.0\nNo\nGreen\nCalothorax lucifer\n\n\n570.0\n75.0\nNo\nBlack\nCampephilus principalis ``` The first four columns are features, and the last column is the label. The first two features are numeric and can take on decimal values. The third feature is binary that can only be \\(1\\) (Yes) or \\(0\\) (No). The fourth feature is an enumeration over the color palette. You may either treat it as categorical data or numeric data, depending on how you want to build the model and what you want to get out of the data. In this example we will use it as categorical data that we only choose it from a list of colors (\\(1\\) — Brown, \\(2\\) — Gray, \\(3\\) — Black, \\(4\\) — Green).\n\n\nThen we are able to transform the above data into the following form:\n```{list-table} Vectorized Bird species data :header-rows: 1\n\n\nFeatures\nLabels\n\n\n\\(\\begin{bmatrix}1001.1 & 125.0 & 0 & 1 \\end{bmatrix}\\)\n\\(1\\)\n\n\n\\(\\begin{bmatrix}3000.7 & 200.0 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}3300.0 & 220.3 & 0 & 2 \\end{bmatrix}\\)\n\\(2\\)\n\n\n\\(\\begin{bmatrix}4100.0 & 136.0 & 1 & 3 \\end{bmatrix}\\)\n\\(3\\)\n\n\n\\(\\begin{bmatrix}3.0 & 11.0 & 0 & 4 \\end{bmatrix}\\)\n\\(4\\)\n\n\n\\(\\begin{bmatrix}570.0 & 75.0 & 0 & 3 \\end{bmatrix}\\)\n\\(5\\) ```\n\n\nThen the Supervised Learning problem is stated as follows: Given the features and the labels, we would like to find a model that can classify future data."
  },
  {
    "objectID": "contents/1/intro.html#parameters-and-hyperparameters",
    "href": "contents/1/intro.html#parameters-and-hyperparameters",
    "title": "1  Introduction",
    "section": "3.2 Parameters and hyperparameters",
    "text": "3.2 Parameters and hyperparameters\nA model parameter is internal to the model and its value is learned from the data.\nA model hyperparameter is external to the model and its value is set by people.\nFor example, assume that we would like to use Logistic regression to fit the data. We set the learning rate is 0.1 and the maximal iteration is 100. After the computations are done, we get a the model\n\\[\ny = \\sigma(0.8+0.7x).\n\\] The two cofficients \\(0.8\\) and \\(0.7\\) are the parameters of the model. The model Logistic regression, the learning rate 0.1 and the maximal iteration 100 are all hyperparametrs. If we change to a different set of hyperparameters, we may get a different model, with a different set of parameters.\nThe details of Logistic regression will be discussed in {numref}Chapter %s<chapter-log-reg>."
  },
  {
    "objectID": "contents/1/intro.html#evaluate-a-machine-learning-model",
    "href": "contents/1/intro.html#evaluate-a-machine-learning-model",
    "title": "1  Introduction",
    "section": "3.3 Evaluate a Machine Learning model",
    "text": "3.3 Evaluate a Machine Learning model\nOnce the model is built, how do we know that it is good or not? The naive idea is to test the model on some brand new data and check whether it is able to get the desired results. The usual way to achieve it is to split the input dataset into three pieces: training set, validation set and test set.\nThe model is initially fit on the training set, with some arbitrary selections of hyperparameters. Then hyperparameters will be changed, and new model is fitted over the training set. Which set of hyperparameters is better? We then test their performance over the validation set. We could run through a lot of different combinations of hyperparameters, and find the best performance over the validation set. After we get the best hyperparameters, the model is selcted, and we fit it over the training set to get our model to use.\nTo compare our model with our models, either our own model using other algorithms, or models built by others, we need some new data. We can no longer use the training set and the validation set since all data in them are used, either for training or for hyperparameters tuning. We need to use the test set to evaluate the “real performance” of our data.\nTo summarize:\n\nTraining set: used to fit the model;\nValidation set: used to tune the hyperparameters;\nTest set: used to check the overall performance of the model.\n\nThe validation set is not always required. If we use cross-validation technique for hyperparameters tuning, like sklearn.model_selection.GridSearchCV(), we don’t need a separated validation set. In this case, we will only need the training set and the test set, and run GridSearchCV over the training set. The cross-validation will be discussed in {numref}Section %s<section-cross-validation>.\nThe sizes and strategies for dataset division depends on the problem and data available. It is often recommanded that more training data should be used. The typical distribution of training, validation and test is \\((6:3:1)\\), \\((7:2:1)\\) or \\((8:1:1)\\). Sometimes validation set is discarded and only training set and test set are used. In this case the distribution of training and test set is usually \\((7:3)\\), \\((8:2)\\) or \\((9:1)\\)."
  },
  {
    "objectID": "contents/1/intro.html#workflow-in-developing-a-machine-learning-application",
    "href": "contents/1/intro.html#workflow-in-developing-a-machine-learning-application",
    "title": "1  Introduction",
    "section": "3.4 Workflow in developing a machine learning application",
    "text": "3.4 Workflow in developing a machine learning application\nThe workflow described below is from {cite:p}Har2012.\n\nCollect data.\nPrepare the input data.\nAnalyze the input data.\nTrain the algorithm.\nTest the algorithm.\nUse it.\n\nIn this course, we will mainly focus on Step 4 as well Step 5. These two steps are where the “core” algorithms lie, depending on the algorithm. We will start from the next Chapter to talk about various Machine Learning algorithms and examples."
  },
  {
    "objectID": "contents/1/intro.html#python-notebook",
    "href": "contents/1/intro.html#python-notebook",
    "title": "1  Introduction",
    "section": "4.1 Python Notebook",
    "text": "4.1 Python Notebook\nWe mainly use Python Notebook (.ipynb) to write documents for this course. Currently all main stream Python IDE support Python Notebook. All of them are not entirely identical but the differences are not huge and you may choose any you like.\nOne of the easiest ways to use Python Notebook is through JupyterLab. The best part about it is that you don’t need to worry about installation and configuration in the first place, and you can directly start to code.\nClick the above link and choose JupyterLab. Then you will see the following page.\n\nThe webapp you just started is called JupyterLite. This is a demo version. The full JupyterLab installation instruction can also be found from the link.\nThere is a small button + under the tab bar. This is the place where you click to start a new cell. You may type codes or markdown documents or raw texts in the cell according to your needs. The drag-down menu at the end of the row which is named Code or Markdown or Raw can help you make the switch. Markdown is a very simple light wighted language to write documents. In most cases it behaves very similar to plain texts. Codes are just regular Python codes (while some other languages are supported). You may either use the triangle button in the menu to execute the codes, or hit shift + enter.\n\nJupyterLite contains a few popular packages. Therefore it is totally ok if you would like to play with some simple things. However since it is an online evironment, it has many limitations. Therefore it is still recommended to set up a local environment once you get familiar with Python Notebook. Please check the following links for some popular choices for notebooks and Python installations in general, either local and online.\n\nJupyter Notebook / JupyterLab\nVS Code\nPyCharm\nGoogle Colab\nAnaconda"
  },
  {
    "objectID": "contents/1/intro.html#python-fundamentals",
    "href": "contents/1/intro.html#python-fundamentals",
    "title": "1  Introduction",
    "section": "4.2 Python fundamentals",
    "text": "4.2 Python fundamentals\nWe will put some very basic Python commands here for you to warm up. More advanced Python knowledge will be covered during the rest of the semester. The main reference for this part is {cite:p}Har2012. ### Indentation Python is using indentation to denote code blocks. It is not convienent to write in the first place, but it forces you to write clean, readable code.\nBy the way, the if and for block are actually straightforward.\n\n:gutter: 2\n:::{grid-item-card} One! {code-block} python if jj < 3:     jj = jj      print(\"It is smaller than 3.\")\n\n:::{grid-item-card} Two! {code-block} python if jj < 3:     jj = jj print(\"It is smaller than 3.\") ::: ::::\n\n:gutter: 2\n:::{grid-item-card} Three! {code-block} python for i in range(3):     i = i + 1     print(i)\n\n:::{grid-item-card} Four! {code-block} python for i in range(3):     i = i + 1 print(i) ::: :::: Please tell the differences between the above codes.\n\n4.2.1 list and dict\nHere are some very basic usage of lists of dictionaries in Python. ```{code-block} python newlist = list() newlist.append(1) newlist.append(‘hello’) print(newlist)\nnewlisttwo = [1, ‘hello’] print(newlisttwo)\nnewdict = dict() newdict[‘one’] = ‘good’ newdict[1] = ‘yes’ print(newdict)\nnewdicttwo = {‘one’: ‘good’, 1: ‘yes’} print(newdicttwo)\n\n\n### Loop through lists\nWhen creating `for` loops we may let Python directly loop through lists. Here is an example. The code is almost self-explained.\n```{code-block} python\nalist = ['one', 2, 'three', 4]\n\nfor item in alist:\n    print(item)\n\n\n4.2.2 Reading files\nThere are a lot of functions that can read files. The basic one is to read any files as a big string. After we get the string, we may parse it based on the structure of the data.\nThe above process sounds complicated. That’s why we have so many different functions reading files. Usually they focus on a certain types of files (e.g. spreadsheets, images, etc..), parse the data into a particular data structure for us to use later.\nI will mention a few examples.\n\ncsv files and excel files Both of them are spreadsheets format. Usually we use pandas.read_csv and pandas.read_excel both of which are from the package pandas to read these two types of files.\nimages Images can be treated as matrices, that each entry represents one pixel. If the image is black/white, it is represented by one matrix where each entry represents the gray value. If the image is colored, it is represented by three matrices where each entry represents one color. To use which three colors depends on the color map. rgb is a popular choice.\nIn this course when we need to read images, we usually use matplotlib.pyplot.imread from the package matplotlib or cv.imread from the package opencv.\n.json files .json is a file format to store dictionary type of data. To read a json file and parse it as a dictionary, we need json.load from the package json.\n\n\n\n4.2.3 Writing files\n\npandas.DataFrame.to_csv\npandas.DataFrame.to_excel\nmatplotlib.pyplot.imsave\ncv.imwrite\njson.dump\n\n\n\n4.2.4 Relative paths\nIn this course, when reading and writing files, please keep all the files using relative paths. That is, only write the path starting from the working directory.\n\nConsider the following tasks:\n\n1. Your working directory is `C:/Users/Xinli/projects/`.\n2. Want to read a file `D:/Files/example.csv`.\n3. Want to generate a file whose name is `result.csv` and put it in a subfoler named `foldername`.\n\nTo do the tasks, don't directly run the code `pd.read_csv('D:/Files/example.csv')`. Instead you should first copy the file to your working directory `C:/Users/Xinli/projects/`, and then run the following code. \n\n```{code-block} python\nimport pandas as pd\n\ndf = pd.read_csv('example.csv')\ndf.to_csv('foldername/result.csv')\n```\nPlease pay attention to how the paths are written.\n\n\n4.2.5 .\n\nclass and packages.\nGet access to attributes and methods\nChaining dots."
  },
  {
    "objectID": "contents/1/intro.html#some-additional-topics",
    "href": "contents/1/intro.html#some-additional-topics",
    "title": "1  Introduction",
    "section": "4.3 Some additional topics",
    "text": "4.3 Some additional topics\n\n4.3.1 Package management and Virtual environment\n\nconda\n\nconda create\n\nconda create --name myenv\nconda create --name myenv python=3.9\nconda create --name myenv --file spec-file.txt\n\nconda install\n\nconda install -c conda-forge numpy\n\nconda activate myenv\nconda list\n\nconda list numpy\nconda list --explicit > spec-file.txt\n\nconda env list\n\npip / venv\n\npython -m venv newenv\nnewenv\\Scripts\\activate\npip install\npip freeze > requirements.txt\npip install -r /path/to/requirements.txt\ndeactivate\n\n\n\n\n4.3.2 Version Control\n\nGit\n\nInstall\ngit config --list\ngit config --global user.name \"My Name\"\ngit config --global user.email \"myemail@example.com\"\n\nGitHub"
  },
  {
    "objectID": "contents/1/intro.html#python-notebook-1",
    "href": "contents/1/intro.html#python-notebook-1",
    "title": "1  Introduction",
    "section": "5.1 Python Notebook",
    "text": "5.1 Python Notebook\n```{exercise} Hello World!\nPlease set up a Python Notebook environment and type print('Hello World!').\n\n\n```{exercise}\n\nPlease set up a Python Notebook and start a new virtual environment and type `print('Hello World!')`."
  },
  {
    "objectID": "contents/1/intro.html#basic-python",
    "href": "contents/1/intro.html#basic-python",
    "title": "1  Introduction",
    "section": "5.2 Basic Python",
    "text": "5.2 Basic Python\n```{exercise} Play with lists :label: ex1helloworld\nPlease complete the following tasks. - Write a for loop to print values from 0 to 4. - Combine two lists ['apple', 'orange'] and ['banana'] using +. - Sort the list ['apple', 'orange', 'banana'] using sorted().\n\n````{solution} ex1helloworld\n:class: dropdown\n\n```{code-block} python\nfor i in range(5):\n    print(i)\n\nnewlist = ['apple', 'orange'] + ['banana']\n\nsorted(['apple', 'orange', 'banana'])\nPlease be careful about the last line. sorted() doesn’t change the original list. It create a new list. There are some Python functions which change the inputed object in-place. Please read documents on all packages you use to get the desired results.\n\n```{exercise} Play with list, dict and pandas.\n:label: ex1list\n\nPlease complete the following tasks.\n- Create a new dictionary `people` with two keys `name` and `age`. The values are all empty list.\n- Add `Tony` to the `name` list in `people`. \n- Add `Harry` to the `name` list in `people`.\n- Add number 100 to the `age` list in `people`.\n- Add number 10 to the `age` list in `people`.\n- Find all the keys of `people` and save them into a list `namelist`.\n- Convert the dictionary `people` to a Pandas DataFrame `df`.\n```\n````{solution} ex1list\n:class: dropdown\n```{code-block} python\nimport pandas as pd\n\npeople = {'name': list(), 'age': list()}\npeople['name'].append('Tony')\npeople['name'].append('Harry')\npeople['age'].append(100)\npeople['age'].append(10)\n\nnamelist = people.keys()\n\ndf = pd.DataFrame(people)\n```\n````{exercise} The dataset iris :label: ex1iris\n{code-block} python from sklearn.datasets import load_iris iris = load_iris() Please explore this dataset. - Please get the features for iris and save it into X as an numpy array. - What is the meaning of these features? - Please get the labels for iris and save it into y as an numpy array. - What is the meaning of labels?\n````{solution} ex1iris\n:class: dropdown\nWe first find that `iris` is a dictionary. Then we can look at all the keys by `iris.keys()`. The interesting keys are `data`, `target`, `target_names` and `feature_names`. We can also read the description of the dataset by looking at `DESCR`. \n```{code-block} python\nX = iris['data']\nprint(iris['feature_names'])\ny = iris['target']\nprint(iris['target'])\n```\nSince the data is already saved as numpy arrays, we don't need to do anything to change its type.\n```{exercise} Play with Pandas :label: ex1pandastitanic Please download the Titanic data file from {Download}here<./assests/datasets/titanic.csv>`. Then follow the instructions to perform the required tasks.\n\nUse pandas.read_csv to read the dataset and save it as a dataframe object df.\nChange the values of the Sex column that male is 0 and female is 1.\nPick the columns Pclass, Sex, Age, Siblings/Spouses Aboard, Parents/Children Aboard and Fare and transform them into a 2-dimensional numpy.ndarray, and save it as X.\nPick the column Survived and transform it into a 1-dimensional numpy.ndarray and save it as y.\n\n````{solution} ex1pandastitanic\n:class: dropdown\n\nNot yet done!"
  },
  {
    "objectID": "contents/2/intro.html",
    "href": "contents/2/intro.html",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "",
    "text": "3 k-Nearest Neighbors Algorithm (k-NN)\nThis data is from sklearn.datasets. This dataset consists of 3 different types of irises’ petal / sepal length / width, stored in a \\(150\\times4\\) numpy.ndarray. We already explored the dataset briefly in the previous chapter. This time we will try to use the feature provided to predict the type of the irises. For the purpose of plotting, we will only use the first two features: sepal length and sepal width.\n````{exercise} Handwritten example :label: ex2handwritten Consider the 1-dimensional data set shown below.\n```{list-table} Dataset :header-rows: 1\n````{solution} ex2titanic :class: dropdown\nNot yet done! ````"
  },
  {
    "objectID": "contents/2/intro.html#ideas",
    "href": "contents/2/intro.html#ideas",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "3.1 Ideas",
    "text": "3.1 Ideas\nAssume that we have a set of labeled data \\(\\{(X_i, y_i)\\}\\) where \\(y_i\\) denotes the label. Given a new data \\(X\\), how do we determine the label of it?\nk-NN algorithm starts from a very straightforward idea. We use the distances from the new data point \\(X\\) to the known data points to identify the label. If \\(X\\) is closer to \\(y_i\\) points, then we will label \\(X\\) as \\(y_i\\).\nLet us take cities and countries as an example. New York and Los Angeles are U.S cities, and Beijing and Shanghai are Chinese cities. Now we would like to consider Tianjin and Russellville. Do they belong to China or U.S? We calculate the distances from Tianjin (resp. Russellville) to all four known cities. Since Tianjin is closer to Beijing and Shanghai comparing to New York and Los Angeles, we classify Tianjin as a Chinese city. Similarly, since Russellville is closer to New York and Los Angeles comparing to Beijing and Shanghai, we classify it as a U.S. city.\n\n\n\n\n\n\n\n\nG\n\n  \n\nBeijing\n\n Beijing   \n\nShanghai\n\n Shanghai   \n\nTianjin\n\n Tianjin   \n\nTianjin->Beijing\n\n  closer   \n\nTianjin->Shanghai\n\n  closer      \n\nNew York\n\n New York   \n\nTianjin->New York\n\n   far away   \n\nLos Angelis\n\n Los Angelis   \n\nTianjin->Los Angelis\n\n   far away   \n\nRussellville\n\n Russellville   \n\nRussellville->Beijing\n\n  far away    \n\nRussellville->Shanghai\n\n  far away   \n\nRussellville->New York\n\n  closer     \n\nRussellville->Los Angelis\n\n  closer  \n\n\n\n\n\nThis naive example explains the idea of k-NN. Here is a more detailed description of the algorithm."
  },
  {
    "objectID": "contents/2/intro.html#the-algorithm",
    "href": "contents/2/intro.html#the-algorithm",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "3.2 The Algorithm",
    "text": "3.2 The Algorithm\n```{prf:algorithm} k-NN Classifier Inputs Given the training data set \\(\\{(X_i, y_i)\\}\\) where \\(X_i=(x_i^1,x_i^2,\\ldots,x_i^n)\\) represents \\(n\\) features and \\(y_i\\) represents labels. Given a new data point \\(\\tilde{X}=(\\tilde{x}^1,\\tilde{x}^2,\\ldots,\\tilde{x}^n)\\).\nOutputs Want to find the best label for \\(\\tilde{X}\\).\n\nCompute the distance from \\(\\tilde{X}\\) to each \\(X_i\\).\nSort all these distances from the nearest to the furthest.\nFind the nearest \\(k\\) data points.\nDetermine the labels for each of these \\(k\\) nearest points, and compute the frenqucy of each labels.\nThe most frequent label is considered to be the label of \\(\\tilde{X}\\).\n\n## Details\n- The distance between two data points are defined by the Euclidean distance:\n  \n\\begin{equation}\ndist\\left((x^j_i)_{j=1}^n, (\\tilde{x}^j)_{j=1}^n\\right)=\\sqrt{\\sum_{j=1}^n(x^j_i-\\tilde{x}^j)^2}.\n\\end{equation}\n  \n- Using linear algebra notations: \n  \n\\begin{equation}\ndist(X_i,\\tilde{X})=\\sqrt{(X_i-\\tilde{X})\\cdot(X_i-\\tilde{X})}.\n\\end{equation}\n\n- All the distances are stored in a $1$-dim numpy array, and we will combine it together with another $1$-dim array that store the labels of each point.\n\n## The codes\n- `argsort`\n- `get`\n- `sorted`\n\n```python\ndef classify_kNN(inX, X, y, k):\n    # create a new 2-d numpy array by copying inX for each row.\n    Xmat = np.tile(np.array([inX]), (X.shape[0], 1))\n    # compute the distance between each row of X and Xmat\n    Dmat = np.sqrt(np.sum((Xmat - X)**2, axis=1))\n    # sort by distance\n    sortedlist = Dmat.argsort()\n    # count the freq. of the first k items\n    k = min(k, len(sortedlist))\n    classCount = dict()\n    for i in sortedlist[:k]:\n        classCount[y[i]] = classCount.get(y[i], 0) + 1\n    # find out the most freqent one\n    sortedCount = sorted(classCount.items(), key=lambda x:x[1],\n                         reverse=True)\n    return sortedCount[0][0]"
  },
  {
    "objectID": "contents/2/intro.html#sklearn-packages",
    "href": "contents/2/intro.html#sklearn-packages",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "3.3 sklearn packages",
    "text": "3.3 sklearn packages\nYou may also directly use the kNN function KNeighborsClassifier packaged in sklearn.neighbors. You may check the description of the function online from here.\nThere are many ways to modify the kNN algorithm. What we just mentioned is the simplest idea. It is correspondent to the argument weights='uniform', algorithm='brute and metric='euclidean'. However due to the implementation details, the results we got from sklearn are still a little bit different from the results produced by our naive codes.\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='brute',\n                             metric='euclidean')\nclf.fit(X_train, y_train)\ny_pred = ckf.predict(X_test)"
  },
  {
    "objectID": "contents/2/intro.html#normalization",
    "href": "contents/2/intro.html#normalization",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "3.4 Normalization",
    "text": "3.4 Normalization\nDifferent features may have different scales. It might be unfair for those features that have small scales. Therefore usually it is better to rescale all the features to make them have similar scales. After examining all the data, we find the minimal value minVal and the range ranges for each column. The normalization formula is:\n\\[X_{norm} = \\frac{X_{original}-minVal}{ranges}.\\]\nWe could also convert the normalized number back to the original value by\n\\[X_{original} = X_{norm} \\times ranges + minVal.\\]\nThe sample codes are listed below.\ndef encodeNorm(X, parameters=None):\n    # parameters contains minVals and ranges\n    if parameters is None:\n        minVals = np.min(X, axis=0)\n        maxVals = np.max(X, axis=0)\n        ranges = np.maximum(maxVals - minVals, np.ones(minVals.size))\n        parameters = {'ranges': ranges, 'minVals': minVals}\n    else:\n        minVals = parameters['minVals']\n        ranges = parameters['ranges']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xnorm = (X - Nmat)/ranges\n    return (Xnorm, parameters)\n\n\ndef decodeNorm(X, parameters):\n    # parameters contains minVals and ranges\n    ranges = parameters['ranges']\n    minVals = parameters['minVals']\n    Nmat = np.tile(minVals, (X.shape[0], 1))\n    Xoriginal = X * ranges + Nmat\n    return Xoriginal"
  },
  {
    "objectID": "contents/2/intro.html#explore-the-dataset",
    "href": "contents/2/intro.html#explore-the-dataset",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "4.1 Explore the dataset",
    "text": "4.1 Explore the dataset\nWe first load the dataset.\n\nfrom sklearn import datasets\niris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target\n\nThen we would like to split the dataset into trainning data and test data. Here we are going to use sklearn.model_selection.train_test_split function. Besides the dataset, we should also provide the propotion of the test set comparing to the whole dataset. We will choose test_size=0.1 here, which means that the size of the test set is 0.1 times the size of the whole dataset. stratify=y means that when split the dataset we want to split respects the distribution of labels in y.\nThe split will be randomly. You may set the argument random_state to be a certain number to control the random process. If you set a random_state, the result of the random process will stay the same. This is for reproducible output across multiple function calls.\nAfter we get the training set, we should also normalize it. All our normalization should be based on the training set. When we want to use our model on some new data points, we will use the same normalization parameters to normalize the data points in interests right before we apply the model. Here since we mainly care about the test set, we could normalize the test set at this stage.\nNote that in the following code, I import functions encodeNorm from assests.codes.knn. You need to modify this part based on your file structure. See here for more details.\n\nfrom sklearn.model_selection import train_test_split\nfrom assests.codes.knn import encodeNorm\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1, stratify=y)\n\nX_train_norm, parameters = encodeNorm(X_train)\nX_test_norm, _ = encodeNorm(X_test, parameters=parameters)\n\nBefore we start to play with k-NN, let us look at the data first. Since we only choose two features, it is able to plot these data points on a 2D plane, with different colors representing different classes.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Plot the scatter plot.\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nscatter = ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train)\n\n# Generate legends.\nlabels = ['setosa', 'versicolor', 'virginica']\nfig.legend(handles=scatter.legend_elements()[0], labels=labels,\n           loc=\"right\", title=\"Labels\")\n\nplt.show()\n\n\n\n\n(section:applyourknn)= ## Apply our k-NN model\nNow let us apply k-NN to this dataset. We first use our codes. Here I use from assests.codes.knn to import our functions since I put all our functions in ./assests/codes/knn.py. Then the poential code is\ny_pred = classify_kNN(X_test, X_train, y_train, k=10)\nNote that the above code is actually wrong. The issue ist that our function classify_kNN can only classify one row of data. To classify many rows, we need to use a for loop.\n\nfrom assests.codes.knn import classify_kNN\n\nn_neighbors = 10\ny_pred = list()\nfor row in X_test_norm:\n    row_pred = classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n    y_pred.append(row_pred)\ny_pred = np.array(y_pred)\n\nWe could use list comprehension to simply the above codes.\n\nfrom assests.codes.knn import classify_kNN\n\nn_neighbors = 10\ny_pred = np.array([classify_kNN(row, X_train_norm, y_train, k=n_neighbors)\n                   for row in X_test_norm])\n\nThis y_pred is the result we got for the test set. We may compare it with the real answer y_test, and calcuate the accuracy.\n\nacc = np.mean(y_pred == y_test)\nprint(acc)\n\n0.7333333333333333"
  },
  {
    "objectID": "contents/2/intro.html#apply-k-nn-model-from-sklearn",
    "href": "contents/2/intro.html#apply-k-nn-model-from-sklearn",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "4.2 Apply k-NN model from sklearn",
    "text": "4.2 Apply k-NN model from sklearn\nNow we would like to use sklearn to reproduce this result. Since our data is prepared, what we need to do is directly call the functions.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nn_neighbors = 10\nclf = KNeighborsClassifier(n_neighbors, weights=\"uniform\", metric=\"euclidean\",\n                           algorithm='brute')\nclf.fit(X_train_norm, y_train)\ny_pred_sk = clf.predict(X_test_norm)\n\nacc = np.mean(y_pred_sk == y_test)\nprint(acc)\n\n0.7333333333333333"
  },
  {
    "objectID": "contents/2/intro.html#using-data-pipeline",
    "href": "contents/2/intro.html#using-data-pipeline",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "4.3 Using data pipeline",
    "text": "4.3 Using data pipeline\nWe may organize the above process in a neater way. After we get a data, the usual process is to apply several transforms to the data before we really get to the model part. Using terminolgies from sklearn, the former are called transforms, and the latter is called an estimator. In this example, we have exactly one tranform which is the normalization. The estimator here we use is the k-NN classifier.\nsklearn provides a standard way to write these codes, which is called pipeline. We may chain the transforms and estimators in a sequence and let the data go through the pipeline. In this example, the pipeline contains two steps: 1. The normalization transform sklearn.preprocessing.MinMaxScaler. When we directly apply it the parameters ranges and minVals and will be recorded automatically, and we don’t need to worry about it when we want to use the same parameters to normalize other data. 2. The k-NN classifier sklearn.neighbors.KNeighborsClassifier. This is the same one as we use previously.\nThe code is as follows. It is a straightforward code. Note that the () after the class in each step of steps is very important. The codes cannot run if you miss it.\nAfter we setup the pipeline, we may use it as other estimators since it is an estimator. Here we may also use the accuracy function provided by sklearn to perform the computation. It is essentially the same as our acc computation.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\n\nsteps = [('scaler', MinMaxScaler()),\n         ('knn', KNeighborsClassifier(n_neighbors, weights=\"uniform\",\n                                      metric=\"euclidean\", algorithm='brute'))]\npipe = Pipeline(steps=steps)\npipe.fit(X_train, y_train)\ny_pipe = pipe.predict(X_test)\nprint(accuracy_score(y_pipe, y_test))\n\n0.7333333333333333"
  },
  {
    "objectID": "contents/2/intro.html#visualize-the-decision-boundary",
    "href": "contents/2/intro.html#visualize-the-decision-boundary",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "4.4 Visualize the Decision boundary",
    "text": "4.4 Visualize the Decision boundary\nUsing the classifier we get above, we are able to classify every points on the plane. This enables us to draw the following plot, which is called the Decision boundary. It helps us to visualize the relations between features and the classes.\nWe use DecisionBoundaryDisplay from sklearn.inspection to plot the decision boundary. The function requires us to have a fitted classifier. We may use the classifier pipe we got above. Note that this classifier should have some build-in structures that our classify_kNN function doesn’t have. We may rewrite our codes to make it work, but this goes out of the scope of this section. This is supposed to be Python programming exercise. We will talk about it in the future if we have enough time.\nWe first plot the dicision boundary using DecisionBoundaryDisplay.from_estimator. Then we plot the points from X_test. From the plot it is very clear which points are misclassified.\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\ndisp = DecisionBoundaryDisplay.from_estimator(\n            pipe, \n            X_train,\n            response_method=\"predict\",\n            plot_method=\"pcolormesh\",\n            xlabel=iris.feature_names[0],\n            ylabel=iris.feature_names[1],\n            alpha=0.5)\ndisp.ax_.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor=\"k\")\ndisp.figure_.set_size_inches((10,7))\n\n\n\n\n(section-cross-validation)= ## k-Fold Cross-Validation\nPreviously we perform a random split and test our model in this case. What would happen if we fit our model on another split? We might get a different accuracy score. So in order to evaluate the performance of our model, it is natual to consider several different split and compute the accuracy socre for each case, and combine all these socres together to generate an index to indicate whehter our model is good or bad. This naive idea is called k-Fold Cross-Validation.\nThe algorithm is described as follows. We first randomly split the dataset into k groups. We use one of them as the test set, and the rest together forming the training set, and use this setting to get an accuracy score. We did this for each group to be chosen as the test set. Then the final score is the mean.\nsklearn provides a function sklearn.model_selection.cross_val_score to perform the above computation. The usage is straightforward, as follows.\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores = cross_val_score(pipe, X, y, cv=5)\nprint(cv_scores)\nprint(np.mean(cv_scores))\n\n[0.66666667 0.8        0.63333333 0.8        0.7       ]\n0.7200000000000001"
  },
  {
    "objectID": "contents/2/intro.html#choosing-a-k-value",
    "href": "contents/2/intro.html#choosing-a-k-value",
    "title": "2  k-Nearest Neighbors algorithm (k-NN)",
    "section": "4.5 Choosing a k value",
    "text": "4.5 Choosing a k value\nIn the previous example we choose k to be 10 as an example. To choose a k value we usually run some test by trying different k and choose the one with the best performance. In this case, best performance means the highest cross-validation score.\nsklearn.model_selection.GridSearchCV provides a way to do this directly. We only need to setup the esitimator, the metric (which is the cross-validation score in this case), and the hyperparameters to be searched through, and GridSearchCV will run the search automatically.\nWe let k go from 1 to 100. The code is as follows.\nNote that parameters is where we set the search space. It is a dictionary. The key is the name of the estimator plus double _ and then plus the name of the parameter.\n\nfrom sklearn.model_selection import GridSearchCV\nn_list = list(range(1, 101))\nparameters = dict(knn__n_neighbors=n_list)\nclf = GridSearchCV(pipe, parameters)\nclf.fit(X, y)\nprint(clf.best_estimator_.get_params()[\"knn__n_neighbors\"])\n\n35\n\n\nAfter we fit the data, the best_estimator_.get_params() can be printed. It tells us that it is best to use 31 neibhours for our model. We can directly use the best estimator by calling clf.best_estimator_.\n\ncv_scores = cross_val_score(clf.best_estimator_, X, y, cv=5)\nprint(np.mean(cv_scores))\n\n0.82\n\n\nThe cross-validation score using k=31 is calculated. This serves as a benchmark score and we may come back to dataset using other methods and compare the scores."
  }
]