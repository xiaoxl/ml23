{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic idea\n",
    "The Logsitic regression is used to predict the probability of a data point belonging to a specific class. It is based on linear regression. The major difference is that logistic regreesion will have an activation function $\\sigma$ at the final stage to change the predicted values of the linear regression to the values that indicate classes. In the case of binary classification, the outcome of $\\sigma$ will be between $0$ and $1$, which is related to the two classes respectively. In this case, the number is interepted as the probability of the data to be in one of the specific class.\n",
    "\n",
    "The model for Logistic regression is as follows:\n",
    "\n",
    "$$\n",
    "p=\\sigma(L(x))=\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\sigma\\left(\\Theta \\hat{x}^T\\right).\n",
    "$$\n",
    "\n",
    "In most cases, this activation function is chosen to be the Sigmoid funciton.\n",
    "\n",
    "## Sigmoid function\n",
    "\n",
    "The *Sigmoid* function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}.\n",
    "$$\n",
    "The graph of the function is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(-6, 6, 1001)\n",
    "y = 1/(1+np.exp(-x))\n",
    "_ = plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main properties of $\\sigma$ are listed below as a Lemma.\n",
    "\n",
    "```{prf:lemma}\n",
    ":label: sigmoid_property\n",
    "The Sigmoid function $\\sigma(z)$ satisfies the following properties.\n",
    "\n",
    "1. $\\sigma(z)\\rightarrow \\infty$ when $z\\mapsto \\infty$.\n",
    "2. $\\sigma(z)\\rightarrow -\\infty$ when $z\\mapsto -\\infty$.\n",
    "3. $\\sigma(0)=0.5$.\n",
    "4. $\\sigma(z)$ is always increasing.\n",
    "5. $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$.\n",
    "\n",
    "```\n",
    "\n",
    "```{prf:proof}\n",
    "We will only look at the last one.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\sigma'(z)&=-\\frac{(1+\\mathrm e^{-z})'}{(1+\\mathrm e^{-z})^2}=\\frac{\\mathrm e^{-z}}{(1+\\mathrm e^{-z})^2}=\\frac{1}{1+\\mathrm e^{-z}}\\frac{\\mathrm e^{-z}}{1+\\mathrm e^{-z}}\\\\\n",
    "&=\\sigma(z)\\left(\\frac{1+\\mathrm e^{-z}}{1+\\mathrm e^{-z}}-\\frac{1}{1+\\mathrm e^{-z}}\\right)=\\sigma(z)(1-\\sigma(z)).\n",
    "\\end{split}\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "## Gradient descent\n",
    "Assume that we would like to minimize a function $J(\\Theta)$, where this $\\Theta$ is an $N$-dim vector. Geometricly, we could treat $J$ as a height function, and it tells us the height of the mountain. Then to minimize $J$ is the same thing as to find the lowest point. One idea is to move towards the lowest point step by step. During each step we only need to lower our current height. After several steps we will be around the lowest point.\n",
    "\n",
    "The geometric meaning of $\\nabla J$ is the direction that $J$ increase the most. Therefore the opposite direction is the one we want to move in. The formula to update $x$ is \n",
    "\n",
    "$$\n",
    "\\Theta_{\\text{new}} = \\Theta_{\\text{old}}-\\alpha \\nabla J(\\Theta_{\\text{old}}),\n",
    "$$\n",
    "where $\\alpha$ is called the *learning rate* which controls how fast you want to learn. Usually if $\\alpha$ is small, the learning tends to be slow and stble, and when $\\alpha$ is big, the learning tends to be fast and unstable.\n",
    "\n",
    "In machine learning, in most cases we would like to formulate the problem in terms of finding the lowest point of a *cost function* $J(\\Theta)$. Then we could start to use Logistic regression to solve it. For binary classification problem, the cost function is defined to be\n",
    "\n",
    "$$\n",
    "J(\\Theta)=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\log(p^{(i)})+(1-y^{(i)})\\log(1-p^{(i)})\\right].\n",
    "$$\n",
    "Here $m$ is the number of data points, $y^{(i)}$ is the labelled result (which is either $0$ or $1$), $p^{(i)}$ is the predicted value (which is between $0$ and $1$). \n",
    "\n",
    "```{note}\n",
    "The algorithm gets its name since we are using the gradient to find a direction to lower our height. \n",
    "```\n",
    "\n",
    "## The Formulas\n",
    "````{prf:theorem}\n",
    "The gradient of $J$ is computed by\n",
    "\n",
    "```{math}\n",
    ":label: eqn-nablaJ\n",
    "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "````{toggle}\n",
    "\n",
    "```{prf:proof}\n",
    "The formula is an application of the chain rule for the multivariable functions.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\dfrac{\\partial p}{\\partial \\theta_k}&=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)=\\dfrac{\\partial}{\\partial \\theta_k}\\sigma(L(\\Theta))\\\\\n",
    "&=\\sigma(L)(1-\\sigma(L))\\dfrac{\\partial}{\\partial \\theta_k}\\left(\\theta_0+\\sum_{j=1}^n\\theta_jx_j\\right)\\\\\n",
    "&=\\begin{cases}\n",
    "p(1-p)&\\text{ if }k=0,\\\\\n",
    "p(1-p)x_k&\\text{ otherwise}.\n",
    "\\end{cases}\n",
    "\\end{split}\n",
    "$$\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\nabla p = \\left(\\frac{\\partial p}{\\partial\\theta_0},\\ldots,\\frac{\\partial p}{\\partial\\theta_n}\\right) = p(1-p)\\hat{x}.\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\nabla \\log(p) = \\frac{\\nabla p}p =\\frac{p(1-p)\\hat{x}}{p}=(1-p)\\hat{x}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla \\log(1-p) = \\frac{-\\nabla p}{1-p} =-\\frac{p(1-p)\\hat{x}}{1-p}=-p\\hat{x}.\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla J& = -\\frac1m\\sum_{i=1}^m\\left[y^{(i)}\\nabla \\log(p^{(i)})+(1-y^{(i)})\\nabla \\log(1-p^{(i)})\\right]\\\\\n",
    "&=-\\frac1m\\sum_{i=1}^m\\left[y^{(i)}(1-p^{(i)})\\hat{x}^{(i)}+(1-y^{(i)})(-p^{(i)}\\hat{x}^{(i)})\\right]\\\\\n",
    "&=-\\frac1m\\sum_{i=1}^m\\left[(y^{(i)}-p^{(i)})\\hat{x}^{(i)}\\right].\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We write $\\hat{x}^{(i)}$ as row vectors, and stack all these row vectors vertically. What we get is a matrix $\\hat{\\textbf X}$ of the size $m\\times (1+n)$. We stack all $y^{(i)}$ (resp. $p^{(i)}$) vectically to get the $m$-dim column vector $\\textbf y$ (resp. $\\textbf p$). \n",
    "\n",
    "Using this notation, the previous formula becomes\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n",
    "$$\n",
    "\n",
    "After the gradient can be computed, we can start to use the gradient descent method. Note that, although $\\Theta$ are not explicitly presented in the formula of $\\nabla J$, this is used to modify $\\Theta$:\n",
    "\n",
    "$$\n",
    "\\Theta_{s+1} = \\Theta_s - \\alpha\\nabla J.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codes\n",
    "We will only talk about using packages. `sklearn` provides two methods to implement the Logistic regression. The API interface is very similar to other models. \n",
    "\n",
    "Note that Logistic regression is very sensitive to the scale of features. Therefore we need to normalize the features before throwing them into the model.\n",
    "\n",
    "Let's still take `iris` as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method is `sklearn.linear_model.LogisticRegression`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "steps = [('normalize', MinMaxScaler()),\n",
    "         ('log', LogisticRegression())]\n",
    "\n",
    "log_reg = Pipeline(steps=steps)\n",
    "log_reg.fit(X_train, y_train)\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method has an option `solver` that will set the way to solve the Logistic regression problem, and there is no \"stochastic gradient descent\" provided. The default solver for this `LogsiticRegression` is `lbfgs` which will NOT be discussed in lectures.\n",
    "\n",
    "The second method is `sklearn.linear_model.SGDClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565217391304348"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "steps = [('normalize', MinMaxScaler()),\n",
    "         ('log', SGDClassifier(loss='log_loss', max_iter=100))]\n",
    "\n",
    "sgd_clf = Pipeline(steps=steps)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method is the one we discussed in lectures. The `log_loss` loss function is the binary entropy function we mentioned in lectures. If you change to other loss functions it will become other models.\n",
    "\n",
    "From the above example, you may notice that `SGDClassifier` doesn't perform as well as `LogisticRegression`. This is due to the algorithm. To make `SGDClassifier` better you need to tune the hyperparameters, like `max_iter`, `learning_rate`/`alpha`, `penalty`, etc..\n",
    "\n",
    "\n",
    "```{note}\n",
    "The argument `warm_start` is used to set whether you want to use your previous model. When set to `True`, it will reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. The default is `False`.  \n",
    "\n",
    "Repeatedly calling `fit` when `warm_start` is `True` can result in a different solution than when calling `fit` a single time because of the way the data is shuffled. \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{note}\n",
    "Note that for both methods, regularization (which will be discussed later) is applied by default.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several important side topics\n",
    "\n",
    "### Epochs\n",
    "We use epoch to describe feeding data into the model. One *Epoch* is when an entire dataset is passed through the model once. When using gradient descent, we tend to run several epochs. The number of maximal epochs is one important hyperparameter of this model.\n",
    "\n",
    "The general idea is that more epochs are better for the score of the model, but it will definitely be slower. In addition, sometimes due to many other factors, after a few epochs, the model becomes stall. To train for more epochs cannot improve the model. In this case you have to turn to other methods.\n",
    "\n",
    "\n",
    "### Batch Gradient Descent vs SGD vs Minibatch\n",
    "Recall the Formula {eq}`eqn-nablaJ`: \n",
    "\n",
    "$$\n",
    "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}.\n",
    "$$\n",
    "We could rewrite this formula:\n",
    "\n",
    "$$\n",
    "\\nabla J =\\frac1m(\\textbf{p}-\\textbf{y})^T\\hat{\\textbf{X}}=\\frac1m\\sum_{i=1}^m\\left[(p^{(i)}-y^{(i)})\\hat{x}^{(i)}\\right].\n",
    "$$\n",
    "This new formula can be understood in the following way: For every data point, we could get one gradient direction. Then $\\nabla J$ is the average of all gradient directions. So this algorithm can be expressed as that compute the gradient for every data points and then take the average, and finally update the parameters once. This algorithm is called *batch gradient descent*. \n",
    "\n",
    "\n",
    "Following the idea, there is another way to update the model. For every data point, we could compute one gradient direction, and we could use the gradient direction to update the parameters of the model. This algorithm is called *stochastic gradient descent*. \n",
    "\n",
    "Then there is an algrothm living in the middle, called *mini-batch gradient descent*. In this case, we will group the data set into a collection of subsets of a fiexed number of training examples. Each subset is called a *mini-batch*, and the fixed number of elements of each mini-batch is called the *batch size*. Using this method, we will just go through mini-batches one at a time, compute the average of the gradient for these data, and then update the parameters of the model after we finish one mini-batch. Assume that the total number of the dataset is `N`, the mini-batch size is `m`. Then there are `N/m` mini-batches, and during one epoch we will update the model `N/m` times.\n",
    "\n",
    "\n",
    "Mini-batch size is one important hyperparameters of this model. Usually the larger the batch size is, the less variance the model has. Then it tends to behave more smoothly, but it will also be slower, and might be stuck to a local minimal. The smaller batch size is more chaotic. It might go faster, but it tends not to converge.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eae2d79809986d0872e4e364459f0c9575ffff27a18380d5ee1c7bc910cc873"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
