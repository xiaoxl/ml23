{
  "hash": "6c6b593dbce0117ba6ab1a38e1869a80",
  "result": {
    "markdown": "# Datasets {.appendix}\n\n\n## the `iris` dataset\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\n## The `breast_cancer` dataset\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\ncancer = load_breast_cancer()\nX = cancer.data\ny = cancer.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\n## the Horse Colic Dataset {#sec-dataset-horsecolic}\nThe data is from the [UCI database](https://archive.ics.uci.edu/ml/datasets/Horse+Colic). The data is loaded as follows. `?` represents missing data.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/horse-colic/horse-colic.data'\ndf = pd.read_csv(url, delim_whitespace=True, header=None)\ndf = df.replace(\"?\", np.NaN)\n```\n:::\n\n\nThe description of the data is listed [here](https://archive.ics.uci.edu/ml/datasets/Horse+Colic). We will preprocess the data according to the descrption.\n\n1. The data tries to predict Column 24. Since Python index starts from 0, in our case we are interested in Column 23.\n2. Column 25-27 (in our case is Column 24-26) use a special code to represent the type of lesion. For simplicity we remove these three columns.\n3. Column 28 (in our case Column 27) is of no significance so we will remove it too.\n4. Column 3 (in our case Column 2) is the IDs of Hospitals which should have very little impact so we will remove it too.\n5. We will fill the missing values with `0`.\n6. We also would like to change the label from `1` and `2` to `0` and `1` for the purpose of Logistic regression.\n\n*This part should be modified if you want to improve the performance of your model.*\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.fillna(0, inplace=True)\ndf.drop(columns=[2, 24, 25, 26, 27], inplace=True)\ndf[23].replace({1: 1, 2: 0}, inplace=True)\nX = df.iloc[:, :-1].to_numpy().astype(float)\ny = df[23].to_numpy().astype(int)\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\n## the Dating dataset\nThe data file can be downloaded from [here](./assests/datasets/datingTestSet2.txt). \n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('datingTestSet2.txt', sep='\\t', header=None)\nX = np.array(df[[0, 1, 2]])\ny = np.array(df[3])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n\n\n\n## The dataset randomly generated\n- `make_moon` dataset\n\nThis is used to generate two interleaving half circles.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\n- `make_gaussian_quantiles` dataset\n\nThis is a generated isotropic Gaussian and label samples by quantile. \n\nThe following code are from [this page](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py). It is used to generate a relative complex dataset by combining two datesets together.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_gaussian_quantiles\nfrom sklearn.model_selection import train_test_split\n\nX1, y1 = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n                                 n_classes=2, random_state=1)\nX2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=300,\n                                 n_features=2, n_classes=2, random_state=1)\nX = np.concatenate((X1, X2))\ny = np.concatenate((y1, -y2 + 1))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\nIt can also be used to generate multiclass dataset.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_gaussian_quantiles\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_gaussian_quantiles(cov=2.0, n_samples=200, n_features=2,\n                               n_classes=4, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\n- `make_classification`\n  \nThis will create a multiclass dataset. Without shuffling, `X` horizontally stacks features in the following order: the primary `n_informative` features, followed by `n_redundant` linear combinations of the informative features, followed by `n_repeated` duplicates, drawn randomly with replacement from the informative and redundant features. \n\nFor more details please see the [official document](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn.datasets.make_classification).\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=2, n_redundant=2, n_repeated=2, n_classes=3, n_clusters_per_class=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n```\n:::\n\n\n## `MNIST` dataset\n\nThere are several versions of the dataset. \n\n- `tensorflow` provides the data with the original split.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport keras_core as keras\n(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing TensorFlow backend\n```\n:::\n:::\n\n\n## `titanic` dataset\nThis is the famuous Kaggle101 dataset. The original data can be download from [the Kaggle page](https://www.kaggle.com/competitions/titanic/data). You may also download the {Download}`training data<./assests/datasets/titanic/train.csv>` and the {Download}`test data<./assests/datasets/titanic/test.csv>` by click the link.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\ndftrain = pd.read_csv('train.csv')\ndftest = pd.read_csv('test.csv')\n```\n:::\n\n\n\n\n::: {.hidden}\n\nThe original is a little bit messy with missing values and mix of numeric data and string data. The above code reads the data into a DataFrame. The following code does some basic of preprocess. This part should be modified if you want to improve the performance of your model.\n\n1. Only select columns: `Pclass`, `Sex`, `Age`, `SibSp`, `Parch`, `Fare`. That is to say, `Name`, `Cabin` and `Embarked` are dropped.\n2. Fill the missing values in column `Age` and `Fare` by `0`.\n3. Replace the column `Sex` by the following map: `{'male': 0, 'female': 1}`.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ndef getnp(df):\n    df['mapSex'] = df['Sex'].map(lambda x: {'male': 0, 'female': 1}[x])\n    dfx = df[['Pclass', 'mapSex', 'Age', 'SibSp', 'Parch', 'Fare']].copy()\n    dfx['Fare'].fillna(0, inplace=True)\n    dfx['Age'].fillna(0, inplace=True)\n    if 'Survived' in df.columns:\n        y = df['Survived'].to_numpy()\n    else:\n        y = None\n    X = dfx.to_numpy()\n    return (X, y)\n\nX_train, y_train = getnp(dftrain)\nX_test, _ = getnp(dftest)\n```\n:::\n\n\nFor the purpose of submitting to Kaggle, after getting `y_pred`, we could use the following file to prepare for the submission file.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef getdf(df, y):\n    df['Survived'] = y\n    return df[['PassengerId', 'Survived']]\n\ngetdf(dftest, y_pred).to_csv('result.csv')\n```\n:::\n\n\n:::\n\n## Plant survival data with salt and microbe treatments\n\nThis dataset is supported by DART SEED grant. It is provided by Dr. Suresh Subedi from ATU. The dataset is about the outcomes of certain treatments applied to plants. We would like to predict whether the plants survive based on the status of the plants and the treatments. The datafile can be downloaded from [here](assests/datasets/plants.xlsx).\n\nWe could use the following code to read the data.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_excel('assests/datasets/plants.xlsx', engine='openpyxl', sheet_name='data')\n```\n:::\n\n\nThere are a few missing values. The missing values in `Outcome_after 12 months` are all `dead`. These are not recorded as `dead` because the cause of the death is more complicated and needs to be studied separatedly. In our case we could simply fill it with `dead`.\n\nThere are two more missing values in `Stem diameter`. For simplicity we drop them directly.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ndf['Outcome_after 12 months'].fillna('dead', inplace=True)\ndf = df.dropna()\n```\n:::\n\n\n::: {.hidden}\n\nThen we would like to transform the data. Here are the rules. \n\n- `Endophyte`: `I+`->`1`, `I-`->`-1`\n- `Treatment`: `Salt`->`1`, `Fresh`->`0`\n- `Tree_Replicate`: `T1`->`1`, `T2`->`2`, `T3`->`3`\n- `Outcome_after 12 months`: `survived`->`1`, `dead`->0\n\nColumn `SN` will be dropped. \n\nFinally we put these together to get the features `X` and the label `y`.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\ndf['Endophyte '] = df['Endophyte '].map({'I+': 1, 'I-': -1})\ndf['Treatment'] = df['Treatment'].map({'Fresh': 0, 'Salt': 1})\ndf['Tree_Replicate'] = df['Tree_Replicate'].str[1].astype(int)\ndf['Outcome_after 12 months'] = df['Outcome_after 12 months'].map({'survived': 1, 'dead': 0})\n\nX = df.iloc[:, 1: -1].to_numpy()\ny = df['Outcome_after 12 months'].to_numpy()\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "intro_files\\figure-pdf"
    ],
    "filters": []
  }
}